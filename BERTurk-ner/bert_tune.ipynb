{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "189272d3-2c7a-4434-8651-3d978c0497a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b3fd5cf5-8176-4617-b00b-7783660e71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('encoded.csv').drop(columns=['Unnamed: 0','Polarities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2d093d90-7d2c-43b7-acd0-324f520959dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[0:2959]\n",
    "test = data[2959:2959+370].reset_index()\n",
    "valid = data[2959+370:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2de72451-fc3d-431a-ac44-1be9c195186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.Dataset.from_pandas(train)\n",
    "test_ds = datasets.Dataset.from_pandas(test)\n",
    "valid_ds = datasets.Dataset.from_pandas(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "adc125aa-61fe-42e3-ae5a-e5fac9f87821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\": train_ds,\n",
    "        \"test\": test_ds,\n",
    "        \"validation\": test_ds\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "054e7136-a689-41b2-80f6-972ce3f549f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = '/karayel/bert-base-turkish-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bc0dd1e1-33a3-401c-a439-a4b3d6a86cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6388a648-ba36-41df-86c2-89774839b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(row):\n",
    "  tokenized_data = tokenizer(row[\"Tokens\"], truncation = True, is_split_into_words = True)\n",
    "\n",
    "  labels = []\n",
    "\n",
    "  for i, label in enumerate(row[\"Tags\"]):\n",
    "    word_ids = tokenized_data.word_ids(i)\n",
    "\n",
    "\n",
    "    label_ids=[]\n",
    "    pre_word_index = None\n",
    "\n",
    "    for word_index in word_ids:\n",
    "      if word_index != pre_word_index:\n",
    "        pre_word_index = word_index\n",
    "        if word_index is None:\n",
    "          label_ids.append(-100)\n",
    "        else:\n",
    "          label_ids.append(label[word_index])\n",
    "\n",
    "      elif word_index is None:\n",
    "        label_ids.append(-100)\n",
    "\n",
    "      else:\n",
    "        l = label[word_index]\n",
    "\n",
    "        if l%2 == 1:\n",
    "          l = l + 1\n",
    "        label_ids.append(l)\n",
    "\n",
    "    labels.append(label_ids)\n",
    "\n",
    "  tokenized_data[\"labels\"] = labels\n",
    "  return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d5b70fd6-0be6-4da4-bd6d-3692a554f961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b3d55360174aec807198ca7d1fbb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b1e03a7a30452c987f8a311f0fbfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7524d417ed44497a9ee8f46cde40e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_list(example):\n",
    "    example['Tokens'] = ast.literal_eval(example['Tokens'])\n",
    "    example['Tags'] = ast.literal_eval(example['Tags'])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(make_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "685117b9-c047-44f2-b49f-dff97743e329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab98b6e2e844de2ad32cc5b434f09ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047d6f33cb6a476a887352dc974460b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d10bcce7fe436699400bf9f08abb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5b64c9a7-a2f4-4a8e-ba6c-bc751a99ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_ds = tokenized_dataset[\"train\"]\n",
    "tokenized_test_ds = tokenized_dataset[\"test\"]\n",
    "tokenized_eval_ds = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "65cbf6b8-0eec-4ff6-85fb-9df24f2b93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9fb1201f-dda5-4ef2-a8c8-385a9f57b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0:'O', 1:'B-ORG', 2:'I-ORG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d570c184-c6d8-40d9-8ecd-016ff57ddaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'O':0, 'B-ORG':1, 'I-ORG':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "dc73b15f-7c92-4b39-90da-db9c0e86f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4f0bbd41-03cf-4bbf-8965-d357b24fd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_ckpt.split(\"/\")[-1] + \"for_ner\"\n",
    "output_dir_name = \"finetuned_turkish_distilbert_for_ner\"\n",
    "output_tokenizer_name = \"bert_tokenizer_ddi\"\n",
    "strategy = \"epoch\"\n",
    "learning_rate = 2e-5\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16\n",
    "num_train_epochs = 3\n",
    "weight_decay = 0.01\n",
    "metric_for_best_model = \"eval_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "64be637a-826e-410d-b113-3e47c50eadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/camp5/karayel/bert-base-turkish-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_ckpt, num_labels= num_labels, label2id=label2id, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8f8e60c9-42f9-483d-bfea-70e33d529ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camp5/karayel/karayel_llm_1/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    model_name,\n",
    "    evaluation_strategy = strategy,\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    per_device_eval_batch_size = per_device_eval_batch_size,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    weight_decay = weight_decay,\n",
    "    logging_strategy = strategy,\n",
    "    logging_first_step = True,\n",
    "    disable_tqdm = False,\n",
    "    report_to = \"none\",\n",
    "    metric_for_best_model = metric_for_best_model,\n",
    "    push_to_hub = False,\n",
    "    save_safetensors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "be86f860-febd-44af-afea-a837b5a4ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metrics = datasets.load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1638c02f-1c4f-4ad8-814a-a87983ed6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds):\n",
    "  pred_logits, labels = preds\n",
    "  pred_logits = np.argmax(pred_logits, axis=-1) ## axis\n",
    "\n",
    "  predicted_labels = []\n",
    "  true_labels = []\n",
    "\n",
    "  predicted_labels = [\n",
    "      [id2label[p_label] for (p_label, t_label) in zip(prediction, label) if t_label != -100] for prediction, label in zip(pred_logits, labels)\n",
    "  ]\n",
    "\n",
    "  true_labels = [\n",
    "      [id2label[t_label] for t_label in label if t_label != -100] for label in labels\n",
    "  ]\n",
    "\n",
    "  scores = metrics.compute(predictions = predicted_labels, references = true_labels)\n",
    "\n",
    "  return {\n",
    "      \"accuracy\": scores[\"overall_accuracy\"],\n",
    "      \"f1 score\": scores[\"overall_f1\"],\n",
    "      ##\"micro F1\": scores[\"micro_f1\"],\n",
    "      ##\"macro F1\": scores[\"macro_f1\"],\n",
    "      \"recall\": scores[\"overall_recall\"],\n",
    "      ##\"micro Recall\": scores[\"micro_recall\"],\n",
    "      ##\"macro Recall\": scores[\"macro_recall\"],\n",
    "      \"precision\": scores[\"overall_precision\"],\n",
    "      ##\"micro Precision\": scores[\"micro_precision\"],\n",
    "      ##\"macro Precision\": scores[\"macro_precision\"]\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e2436f5f-0115-4e20-99f8-cff3c50c9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = tokenized_train_ds,\n",
    "    eval_dataset = tokenized_eval_ds,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "593ace1b-e808-4807-85c5-fb686281663a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='555' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [555/555 02:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.085003</td>\n",
       "      <td>0.980393</td>\n",
       "      <td>0.709265</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.651846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.079098</td>\n",
       "      <td>0.982304</td>\n",
       "      <td>0.739738</td>\n",
       "      <td>0.820821</td>\n",
       "      <td>0.673235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.085583</td>\n",
       "      <td>0.980603</td>\n",
       "      <td>0.711297</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.664062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f8c21ebe-bc8b-458b-8998-b82de231e231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =  1714660GF\n",
      "  train_loss               =     0.0104\n",
      "  train_runtime            = 0:02:24.54\n",
      "  train_samples_per_second =     61.414\n",
      "  train_steps_per_second   =       3.84\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"train\", train_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c017ee06-f1c1-4aa9-9fe5-c5ef6ca23067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_accuracy           =     0.9806\n",
      "  eval_f1 score           =     0.7113\n",
      "  eval_loss               =     0.0856\n",
      "  eval_precision          =     0.6641\n",
      "  eval_recall             =     0.7658\n",
      "  eval_runtime            = 0:00:01.67\n",
      "  eval_samples_per_second =    221.489\n",
      "  eval_steps_per_second   =     14.367\n"
     ]
    }
   ],
   "source": [
    "output_metrics = trainer.evaluate()\n",
    "trainer.log_metrics(\"eval\", output_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de5abd-5919-4abf-b782-9ee8f046afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_contiguous(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "472cf2d9-e934-4ae6-b16d-5e12141a0c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "44cdab03-4578-4964-9a1d-5ed811422f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "00078b12-b451-424c-bbeb-da6af62f94e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_tokenizer/tokenizer_config.json',\n",
       " './bert_tokenizer/special_tokens_map.json',\n",
       " './bert_tokenizer/vocab.txt',\n",
       " './bert_tokenizer/added_tokens.json',\n",
       " './bert_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./bert_tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karayel_llm_1",
   "language": "python",
   "name": "karayel_llm_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
