{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:22.340108Z",
     "iopub.status.busy": "2024-08-07T07:33:22.339460Z",
     "iopub.status.idle": "2024-08-07T07:33:36.442387Z",
     "shell.execute_reply": "2024-08-07T07:33:36.441241Z",
     "shell.execute_reply.started": "2024-08-07T07:33:22.340058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.42.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:36.444583Z",
     "iopub.status.busy": "2024-08-07T07:33:36.444228Z",
     "iopub.status.idle": "2024-08-07T07:33:41.926873Z",
     "shell.execute_reply": "2024-08-07T07:33:41.925809Z",
     "shell.execute_reply.started": "2024-08-07T07:33:36.444550Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:41.929397Z",
     "iopub.status.busy": "2024-08-07T07:33:41.928243Z",
     "iopub.status.idle": "2024-08-07T07:33:41.967842Z",
     "shell.execute_reply": "2024-08-07T07:33:41.966407Z",
     "shell.execute_reply.started": "2024-08-07T07:33:41.929362Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T09:04:30.380767Z",
     "iopub.status.busy": "2024-08-07T09:04:30.380078Z",
     "iopub.status.idle": "2024-08-07T09:04:32.810107Z",
     "shell.execute_reply": "2024-08-07T09:04:32.809102Z",
     "shell.execute_reply.started": "2024-08-07T09:04:30.380729Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1\",\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:56.107035Z",
     "iopub.status.busy": "2024-08-07T07:33:56.106695Z",
     "iopub.status.idle": "2024-08-07T07:33:56.114308Z",
     "shell.execute_reply": "2024-08-07T07:33:56.113209Z",
     "shell.execute_reply.started": "2024-08-07T07:33:56.107002Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:56.116474Z",
     "iopub.status.busy": "2024-08-07T07:33:56.115805Z",
     "iopub.status.idle": "2024-08-07T07:33:58.176215Z",
     "shell.execute_reply": "2024-08-07T07:33:58.175137Z",
     "shell.execute_reply.started": "2024-08-07T07:33:56.116428Z"
    }
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:58.178754Z",
     "iopub.status.busy": "2024-08-07T07:33:58.177836Z",
     "iopub.status.idle": "2024-08-07T07:33:58.387814Z",
     "shell.execute_reply": "2024-08-07T07:33:58.386537Z",
     "shell.execute_reply.started": "2024-08-07T07:33:58.178715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:58.389454Z",
     "iopub.status.busy": "2024-08-07T07:33:58.389078Z",
     "iopub.status.idle": "2024-08-07T07:33:58.464173Z",
     "shell.execute_reply": "2024-08-07T07:33:58.463231Z",
     "shell.execute_reply.started": "2024-08-07T07:33:58.389426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/train-data-2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source directory\n",
    "import shutil\n",
    "\n",
    "source_dir = \"\"\n",
    "\n",
    "# Destination directory\n",
    "destination_dir = \"\"\n",
    "\n",
    "shutil.copytree(source_dir, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:58.465909Z",
     "iopub.status.busy": "2024-08-07T07:33:58.465538Z",
     "iopub.status.idle": "2024-08-07T07:33:59.281841Z",
     "shell.execute_reply": "2024-08-07T07:33:59.280816Z",
     "shell.execute_reply.started": "2024-08-07T07:33:58.465878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed699b230f764530bf6f2d7be38408f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOAD AND STURCTURE DATA\n",
    "data = load_dataset('csv', data_files=\"\")\n",
    "data = data.remove_columns('Unnamed: 0')\n",
    "data = data['train'].train_test_split(test_size=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:33:59.285548Z",
     "iopub.status.busy": "2024-08-07T07:33:59.285115Z",
     "iopub.status.idle": "2024-08-07T07:34:02.169379Z",
     "shell.execute_reply": "2024-08-07T07:34:02.168052Z",
     "shell.execute_reply.started": "2024-08-07T07:33:59.285509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b29a50eecc24e36a953d899abae1a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/3256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bb68ed9ad4483297474cab61a99c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f14a2fae1e441478a25954693dd82e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e93bd60b6a4ec693f2148b469c7b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data = data.map(lambda samples: tokenizer(samples['instruct_ytu']), batched=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"instruct\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = data.map(tokenize_function, batched=True, num_proc=2, remove_columns=['instruct_ytu'])\n",
    "\n",
    "def copy_input_ids(example):\n",
    "    example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(copy_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:34:02.171515Z",
     "iopub.status.busy": "2024-08-07T07:34:02.171105Z",
     "iopub.status.idle": "2024-08-07T07:34:02.180614Z",
     "shell.execute_reply": "2024-08-07T07:34:02.179319Z",
     "shell.execute_reply.started": "2024-08-07T07:34:02.171477Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:34:10.471331Z",
     "iopub.status.busy": "2024-08-07T07:34:10.470428Z",
     "iopub.status.idle": "2024-08-07T07:34:10.483652Z",
     "shell.execute_reply": "2024-08-07T07:34:10.481324Z",
     "shell.execute_reply.started": "2024-08-07T07:34:10.471273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2949120 || all params: 776980480 || trainable%: 0.37956165900075123\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:34:11.237040Z",
     "iopub.status.busy": "2024-08-07T07:34:11.236057Z",
     "iopub.status.idle": "2024-08-07T07:34:27.479443Z",
     "shell.execute_reply": "2024-08-07T07:34:27.478574Z",
     "shell.execute_reply.started": "2024-08-07T07:34:11.236999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 07:34:16.270702: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-07 07:34:16.270822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-07 07:34:16.398579: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "trainer = transformers.Trainer(\n",
    "    \n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    \n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=36,\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=32,\n",
    "        #warmup_steps=100,\n",
    "        \n",
    "        evaluation_strategy = \"steps\",\n",
    "        logging_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        \n",
    "        eval_steps=2,\n",
    "        logging_steps=2,\n",
    "        save_steps=2,\n",
    "        \n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=10,\n",
    "        \n",
    "        learning_rate=1e-3,\n",
    "        output_dir='outputs',\n",
    "        auto_find_batch_size=True\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:34:34.041395Z",
     "iopub.status.busy": "2024-08-07T07:34:34.040782Z",
     "iopub.status.idle": "2024-08-07T07:34:34.045594Z",
     "shell.execute_reply": "2024-08-07T07:34:34.044614Z",
     "shell.execute_reply.started": "2024-08-07T07:34:34.041363Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T07:34:34.356334Z",
     "iopub.status.busy": "2024-08-07T07:34:34.355502Z",
     "iopub.status.idle": "2024-08-07T08:47:45.797256Z",
     "shell.execute_reply": "2024-08-07T08:47:45.796386Z",
     "shell.execute_reply.started": "2024-08-07T07:34:34.356272Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 1:12:31, Epoch 33/36]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.128000</td>\n",
       "      <td>4.224212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.129500</td>\n",
       "      <td>3.376271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.319600</td>\n",
       "      <td>2.847066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.876100</td>\n",
       "      <td>2.636115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.713800</td>\n",
       "      <td>2.558121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.595800</td>\n",
       "      <td>2.506381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.539000</td>\n",
       "      <td>2.482688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.501200</td>\n",
       "      <td>2.460970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.493200</td>\n",
       "      <td>2.445740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.459100</td>\n",
       "      <td>2.436125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.446100</td>\n",
       "      <td>2.430716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.431600</td>\n",
       "      <td>2.422845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.438400</td>\n",
       "      <td>2.415482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.429600</td>\n",
       "      <td>2.409716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.394400</td>\n",
       "      <td>2.404946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.410200</td>\n",
       "      <td>2.399820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.407200</td>\n",
       "      <td>2.394422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.381200</td>\n",
       "      <td>2.390926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.381000</td>\n",
       "      <td>2.388318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.383100</td>\n",
       "      <td>2.385964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.375300</td>\n",
       "      <td>2.383040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.381100</td>\n",
       "      <td>2.380379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.355100</td>\n",
       "      <td>2.378117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.355700</td>\n",
       "      <td>2.376234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.351200</td>\n",
       "      <td>2.374671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.365400</td>\n",
       "      <td>2.372950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.347200</td>\n",
       "      <td>2.371562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.336500</td>\n",
       "      <td>2.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.346600</td>\n",
       "      <td>2.368849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.336900</td>\n",
       "      <td>2.367136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.344000</td>\n",
       "      <td>2.365740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.328300</td>\n",
       "      <td>2.364845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.330500</td>\n",
       "      <td>2.364064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.331300</td>\n",
       "      <td>2.363160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.320000</td>\n",
       "      <td>2.361990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.310300</td>\n",
       "      <td>2.361019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.326700</td>\n",
       "      <td>2.360286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.320400</td>\n",
       "      <td>2.359596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.324700</td>\n",
       "      <td>2.358797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.286700</td>\n",
       "      <td>2.357841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.303700</td>\n",
       "      <td>2.357372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.313400</td>\n",
       "      <td>2.356956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.307300</td>\n",
       "      <td>2.356436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.300200</td>\n",
       "      <td>2.355954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.298900</td>\n",
       "      <td>2.355558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.298500</td>\n",
       "      <td>2.355246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.295200</td>\n",
       "      <td>2.354889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.293000</td>\n",
       "      <td>2.354620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.290200</td>\n",
       "      <td>2.354390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.296000</td>\n",
       "      <td>2.354204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>2.298600</td>\n",
       "      <td>2.353987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2.302100</td>\n",
       "      <td>2.353845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>2.280800</td>\n",
       "      <td>2.353763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>2.284800</td>\n",
       "      <td>2.353737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=108, training_loss=2.477675053808424, metrics={'train_runtime': 4389.7274, 'train_samples_per_second': 26.702, 'train_steps_per_second': 0.025, 'total_flos': 3.013657596002304e+16, 'train_loss': 2.477675053808424, 'epoch': 33.88235294117647})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T08:56:18.883862Z",
     "iopub.status.busy": "2024-08-07T08:56:18.882791Z",
     "iopub.status.idle": "2024-08-07T08:56:23.290197Z",
     "shell.execute_reply": "2024-08-07T08:56:23.289049Z",
     "shell.execute_reply.started": "2024-08-07T08:56:18.883828Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lora_33.pt')\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5501273,
     "sourceId": 9114344,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
