{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "444cf79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20. sayfa taranıyor...\n",
      "20. sayfa tarandı. 22 veri çekildi.\n",
      "21. sayfa taranıyor...\n",
      "21. sayfa tarandı. 23 veri çekildi.\n",
      "22. sayfa taranıyor...\n",
      "22. sayfada veri çekilemedi.\n",
      "23. sayfa taranıyor...\n",
      "23. sayfa tarandı. 24 veri çekildi.\n",
      "24. sayfa taranıyor...\n",
      "24. sayfada veri çekilemedi.\n",
      "25. sayfa taranıyor...\n",
      "25. sayfa tarandı. 22 veri çekildi.\n",
      "26. sayfa taranıyor...\n",
      "26. sayfa tarandı. 22 veri çekildi.\n",
      "27. sayfa taranıyor...\n",
      "27. sayfa tarandı. 22 veri çekildi.\n",
      "28. sayfa taranıyor...\n",
      "28. sayfa tarandı. 24 veri çekildi.\n",
      "29. sayfa taranıyor...\n",
      "29. sayfa tarandı. 21 veri çekildi.\n",
      "30. sayfa taranıyor...\n",
      "30. sayfa tarandı. 23 veri çekildi.\n",
      "31. sayfa taranıyor...\n",
      "31. sayfa tarandı. 23 veri çekildi.\n",
      "32. sayfa taranıyor...\n",
      "32. sayfa tarandı. 24 veri çekildi.\n",
      "33. sayfa taranıyor...\n",
      "33. sayfa tarandı. 23 veri çekildi.\n",
      "34. sayfa taranıyor...\n",
      "34. sayfa tarandı. 23 veri çekildi.\n",
      "35. sayfa taranıyor...\n",
      "35. sayfa tarandı. 23 veri çekildi.\n",
      "36. sayfa taranıyor...\n",
      "36. sayfa tarandı. 23 veri çekildi.\n",
      "37. sayfa taranıyor...\n",
      "37. sayfa tarandı. 21 veri çekildi.\n",
      "38. sayfa taranıyor...\n",
      "38. sayfa tarandı. 23 veri çekildi.\n",
      "39. sayfa taranıyor...\n",
      "39. sayfa tarandı. 24 veri çekildi.\n",
      "40. sayfa taranıyor...\n",
      "40. sayfa tarandı. 23 veri çekildi.\n",
      "41. sayfa taranıyor...\n",
      "41. sayfa tarandı. 24 veri çekildi.\n",
      "42. sayfa taranıyor...\n",
      "42. sayfa tarandı. 24 veri çekildi.\n",
      "43. sayfa taranıyor...\n",
      "43. sayfa tarandı. 22 veri çekildi.\n",
      "44. sayfa taranıyor...\n",
      "44. sayfa tarandı. 22 veri çekildi.\n",
      "45. sayfa taranıyor...\n",
      "45. sayfa tarandı. 24 veri çekildi.\n",
      "46. sayfa taranıyor...\n",
      "46. sayfa tarandı. 22 veri çekildi.\n",
      "47. sayfa taranıyor...\n",
      "47. sayfa tarandı. 21 veri çekildi.\n",
      "48. sayfa taranıyor...\n",
      "48. sayfa tarandı. 22 veri çekildi.\n",
      "49. sayfa taranıyor...\n",
      "49. sayfa tarandı. 20 veri çekildi.\n",
      "50. sayfa taranıyor...\n",
      "50. sayfa tarandı. 22 veri çekildi.\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m son_sayfa \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[39m# Jupyter Notebook içinde çalıştırmak için\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[39mawait\u001b[39;00m scrape_sikayetler(marka, ilk_sayfa, son_sayfa)\n",
      "Cell \u001b[1;32mIn[8], line 57\u001b[0m, in \u001b[0;36mscrape_sikayetler\u001b[1;34m(marka, ilk_sayfa, son_sayfa)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)  \u001b[39m# Her şikayet sayfası çekiminden sonra bekleme süresi\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m. sayfa tarandı. \u001b[39m\u001b[39m{\u001b[39;00msayfada_kazinan\u001b[39m}\u001b[39;00m\u001b[39m veri çekildi.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)  \u001b[39m# Sayfalar arasında bekleme süresi\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbitti\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:639\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(delay, result)\u001b[0m\n\u001b[0;32m    635\u001b[0m h \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39mcall_later(delay,\n\u001b[0;32m    636\u001b[0m                     futures\u001b[39m.\u001b[39m_set_result_unless_cancelled,\n\u001b[0;32m    637\u001b[0m                     future, result)\n\u001b[0;32m    638\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m future\n\u001b[0;32m    640\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m     h\u001b[39m.\u001b[39mcancel()\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# temel sikayetvar veri kazıma\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "sikayetler = pd.DataFrame(columns=['Marka', 'Sayfa Sayisi', 'Link', 'Sikayet'])\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            if response.status == 200:\n",
    "                return await response.text()\n",
    "            else:\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def marka_sayfasi(session, marka, sayfa):\n",
    "    url = f'https://www.sikayetvar.com/{marka}?page={sayfa}'\n",
    "    html = await fetch(session, url)\n",
    "    if html:\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "    return None\n",
    "\n",
    "async def sikayet_sayfasi(session, link):\n",
    "    url = f'https://www.sikayetvar.com{link}'\n",
    "    html = await fetch(session, url)\n",
    "    if html:\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "    return None\n",
    "\n",
    "async def scrape_sikayetler(marka, ilk_sayfa, son_sayfa):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "\n",
    "        for i in range(ilk_sayfa, son_sayfa + 1):\n",
    "            print(f\"{i}. sayfa taranıyor...\")\n",
    "            soup = await marka_sayfasi(session, marka, i)\n",
    "            if not soup:\n",
    "                print(f\"{i}. sayfada veri çekilemedi.\")\n",
    "                await asyncio.sleep(2)  \n",
    "                continue\n",
    "\n",
    "            sikayet_linkler = [l['href'] for l in soup.find_all('a', class_='complaint-layer') if l['href'].count(marka) > 0]\n",
    "            sayfada_kazinan = len(sikayet_linkler)\n",
    "            \n",
    "            tasks = [sikayet_sayfasi(session, j) for j in sikayet_linkler]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "\n",
    "            for soup_sikayet, j in zip(results, sikayet_linkler):\n",
    "                if soup_sikayet and soup_sikayet.find('div', class_=\"complaint-detail-description\"):\n",
    "                    text = \" \".join(soup_sikayet.find('div', class_=\"complaint-detail-description\").text.split(\" \")[1:])\n",
    "                    sikayetler.loc[len(sikayetler)] = [marka, i, j, text.strip()]\n",
    "                await asyncio.sleep(1)  # Her şikayet sayfası çekiminden sonra bekleme süresi\n",
    "\n",
    "            print(f\"{i}. sayfa tarandı. {sayfada_kazinan} veri çekildi.\")\n",
    "            await asyncio.sleep(2)  # Sayfalar arasında bekleme süresi\n",
    "        \n",
    "        print(\"bitti\")\n",
    "\n",
    "marka = 'apple'\n",
    "ilk_sayfa = 20\n",
    "son_sayfa = 50\n",
    "\n",
    "await scrape_sikayetler(marka, ilk_sayfa, son_sayfa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4d729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sikayetler.to_csv('apple-sikayetvar1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3b74b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tüm dosyalar başarıyla birleştirildi ve 'tum_sikayetler.csv' dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   #csv leri birleştirme\n",
    "\n",
    "dosyalar = ['sikayetler-13.csv', 'sikayetler-12.csv', 'sikayetler-11.csv', 'sikayetler-14.csv']\n",
    "\n",
    "birlesik_df = pd.DataFrame()\n",
    "\n",
    "for dosya in dosyalar:\n",
    "    df = pd.read_csv(dosya)\n",
    "    birlesik_df = pd.concat([birlesik_df, df], ignore_index=True)\n",
    "\n",
    "birlesik_df.to_csv('tum_sikayetler.csv', index=False)\n",
    "\n",
    "print(\"Tüm dosyalar başarıyla birleştirildi ve 'tum_sikayetler.csv' dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17573392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya başarıyla düzenlendi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"apple-sikayetvar.csv\")\n",
    "\n",
    "df = df.drop_duplicates(subset=['Sikayet'])\n",
    "\n",
    "df.to_csv(\"sikayetvar-paycell_duzenlenmis.csv\", index=False)\n",
    "\n",
    "print(\"Dosya başarıyla düzenlendi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e752f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosyalar başarıyla birleştirildi.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "paycell_df = pd.read_csv(\"steam-sikayetvar.csv\")\n",
    "tum_sikayetler_df = pd.read_csv(\"paycell1.csv\")\n",
    "\n",
    "paycell_df.rename(columns={'Şikayet': 'Metin'}, inplace=True)\n",
    "\n",
    "\n",
    "paycell_df.to_csv(\"steam-sikayetvar-guncellenmis.csv\", index=False)\n",
    "\n",
    "print(\"Dosyalar başarıyla birleştirildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d455c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "csv_file_path = r'C:\\Users\\Administrator\\Downloads\\apple-sikayetvar1.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "sikayetler = df['Sikayet'].tolist()\n",
    "\n",
    "data = [{\"sentence\": sikayet} for sikayet in sikayetler]\n",
    "\n",
    "with open('unlabeled_apple.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76d2311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dönüştürülmüş veriler 'turkcell-superonline-fiberinternet-labeled.json' dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def transform_data(data):\n",
    "    transformed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        sentence = item['sentence']\n",
    "        results = item['results']\n",
    "        \n",
    "        entity_list = [result['entity'] for result in results]\n",
    "        \n",
    "        results_list = [{'entity': result['entity'], 'sentiment': result['sentiment']} for result in results]\n",
    "        \n",
    "        transformed_item = {\n",
    "            'sentence': sentence,\n",
    "            'entity_list': entity_list,\n",
    "            'results': results_list\n",
    "        }\n",
    "        \n",
    "        transformed_data.append(transformed_item)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "input_file = 'turkcell-superonline-fiberinternet.json'\n",
    "output_file = 'turkcell-superonline-fiberinternet-labeled.json'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "transformed_data = transform_data(data)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(transformed_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Dönüştürülmüş veriler '{output_file}' dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af1fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yanlış veya eksik etiketlenmiş verilerin kontrolü için\n",
    "\n",
    "import json\n",
    "\n",
    "with open('dosya.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for item in data:\n",
    "    if 'sentence' not in item:\n",
    "        item['sentence'] = \"default sentence\"\n",
    "\n",
    "    if 'results' not in item:\n",
    "        item['results'] = [{\"entity\": \"default entity\", \"sentiment\": \"default sentiment\"}]\n",
    "    else:\n",
    "        if 'entity_list' not in item:\n",
    "            item['entity_list'] = [result['entity'] for result in item['results'] if 'entity' in result]\n",
    "\n",
    "        sentence = item['sentence'].lower()\n",
    "        sentence_words = sentence.split()\n",
    "\n",
    "        halusinations = []\n",
    "        results_entities = [result['entity'].lower() for result in item['results'] if 'entity' in result]\n",
    "        entity_list_entities = [entity.lower() for entity in item['entity_list']]\n",
    "\n",
    "        for result in item['results']:\n",
    "            if 'entity' not in result:\n",
    "                result['entity'] = \"default entity\"\n",
    "            if 'sentiment' not in result:\n",
    "                result['sentiment'] = \"default sentiment\"       # olmayan result attributelerine default değerini atıyor.\n",
    "            entity = result['entity'].lower()\n",
    "            if entity not in sentence:\n",
    "                result['halusinasyon'] = True\n",
    "                halusinations.append(result['entity'])\n",
    "        \n",
    "        for entity in item['entity_list']:\n",
    "            entity_lower = entity.lower()\n",
    "            if entity_lower not in sentence:\n",
    "                if 'halusinasyon' not in item:\n",
    "                    item['halusinasyon'] = []\n",
    "                item['halusinasyon'].append(entity)\n",
    "                if entity not in halusinations:\n",
    "                    halusinations.append(entity)\n",
    "\n",
    "        for entity in entity_list_entities:\n",
    "            if entity not in results_entities:      \n",
    "                if 'halusinasyon' not in item:\n",
    "                    item['halusinasyon'] = []\n",
    "                item['halusinasyon'].append(entity)\n",
    "                if entity not in halusinations:\n",
    "                    halusinations.append(entity)\n",
    "\n",
    "        if halusinations:\n",
    "            item['halusinasyon'] = halusinations\n",
    "\n",
    "with open('yeniDosya.json', 'w', encoding='utf-8') as file: # yeni kaydedilecek dosya\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
